\documentclass[dtu]{dtuarticle}
\usepackage{parskip} % use enters instead of indents

\newcommand{\todo}[1]{\color{red}[TODO: #1]\color{black}}
\newcommand*\chem[1]{\ensuremath{\mathrm{#1}}}
\usepackage{amsmath}
\usepackage{bm} % bold ITALIC math (for vectors!)
\usepackage{siunitx}
\sisetup{
	per-mode=fraction,
	detect-weight = true,
	detect-family = true,
	separate-uncertainty=true, % Dit is voor de plus-minus
	output-decimal-marker={.}
}

\usepackage{subcaption}

\title{Machine Learning Project 2}
\subtitle{Supervised Learning: Classification and Regression}
\author{Group 94}
\course{02452 Machine Learning}
\address{
	DTU Compute \\
	Fall 2025
}
\date{\today}


\begin{document}

	\maketitle

	\begin{table}[h!]
		\renewcommand{\arraystretch}{1.2}
		\centering
		\begin{subtable}{\textwidth}
			\centering
			\begin{tabular}{l | l}
				\textbf{Name}                 & \textbf{Student number} \\ \hline\hline
				Vincent Van Schependom        & s251739                 \\ \hline
				Diego Armando Mijares Ledezma & s251777                 \\ \hline
				Albert Joe Jensen             & s204601
			\end{tabular}
			\caption{Group members.}
			\label{table:members}
		\end{subtable}
		\\ \vspace*{0.5cm}
		\begin{subtable}{\textwidth}
			\centering
			\begin{tabular}{l | *{3}{|r}}
				\textbf{Task}                 & \textbf{Vincent} & \textbf{Diego} & \textbf{Albert} \\ \hline\hline
				Training \& test loops & 0\%              &            0\% &             0\% \\ \hline
				Coding visualisations         & 0\%              &            0\% &             0\% \\ \hline
				Section 3                     & 0\%              &            0\% &             0\% \\ \hline
				\LaTeX                        & 0\%              &            0\% &             0\%
			\end{tabular}
			\caption{Contributions \& responsabilities table.}
			\label{table:contributions}
		\end{subtable}
		\caption{Group information \& work distribution.}
	\end{table}

	\section*{Introduction}

	The objective of this report is to apply the methods that were discussed during the second
	section of the course \textit{Machine Learning} \cite{book} to a chosen dataset. The aim is to perform
	relevant regression and classification to the data.

	The particular dataset that is being investigated is -- just like in Project 1 -- the \textit{Glass Identification} dataset from 1987 by B. German \cite{dataset}. Table \ref{table:members} lists our full names and student numbers, while Table \ref{table:contributions} shows an overview of the contribution of each team member.

	\tableofcontents

	\newpage

	\section{Regression}

	\todo{Make visualisations of the results in Table ref{table:e-test-regression}}

	\begin{table}
		\centering
		\renewcommand{\arraystretch}{1.2}
		\begin{tabular}{| r || l | l || l | l || l |}
			\hline
			\textbf{Outer fold} & \multicolumn{2}{c||}{\textbf{ANN}} & \multicolumn{2}{c||}{\textbf{Linear regression}} & \textbf{Baseline}     \\ \hline\hline
			\(i\)               & \(h_i^*\) & \(E_i^{\text{test}}\)  & \(\lambda_i^*\) & \(E_i^{\text{test}}\)          & \(E_i^{\text{test}}\) \\ \hline
			1                   & 128       & \SI{ 2.847e-02 }{}     & \SI{ 0.01 }{}   & \SI{ 9.516e-14 }{}             & \SI{ 9.893e-06 }{}    \\
			2                   & 128       & \SI{ 1.572e-02 }{}     & \SI{ 0.01 }{}   & \SI{ 3.239e-13 }{}             & \SI{ 7.295e-06 }{}    \\
			3                   & 128       & \SI{ 7.582e-02 }{}     & \SI{ 0.01 }{}   & \SI{ 7.883e-13 }{}             & \SI{ 7.229e-06 }{}    \\
			4                   & 256       & \SI{ 1.983e-02 }{}     & \SI{ 0.01 }{}   & \SI{ 1.095e-13 }{}             & \SI{ 8.142e-06 }{}    \\
			5                   & 128       & \SI{ 2.452e-02 }{}     & \SI{ 0.01 }{}   & \SI{ 2.309e-13 }{}             & \SI{ 3.843e-06 }{}    \\
			6                   & 128       & \SI{ 2.165e-02 }{}     & \SI{ 0.01 }{}   & \SI{ 1.261e-13 }{}             & \SI{ 4.273e-06 }{}    \\
			7                   & 128       & \SI{ 1.983e-02 }{}     & \SI{ 0.01 }{}   & \SI{ 4.525e-13 }{}             & \SI{ 8.609e-06 }{}    \\
			8                   & 128       & \SI{ 1.222e-02 }{}     & \SI{ 0.01 }{}   & \SI{ 1.560e-13 }{}             & \SI{ 1.072e-05 }{}    \\
			9                   & 256       & \SI{ 5.593e-02 }{}     & \SI{ 0.01 }{}   & \SI{ 6.452e-13 }{}             & \SI{ 1.782e-05 }{}    \\
			10                  & 256       & \SI{ 9.549e-02 }{}     & \SI{ 0.01 }{}   & \SI{ 7.505e-14 }{}             & \SI{ 1.526e-05 }{}    \\ \hline
		\end{tabular}
		\caption{Summary of two-level cross validation for predicting \todo{see Python script}. Hyperparameters and test errors $E_i^\text{test}$ on $\mathcal{D}_i^\text{test}$ per outer fold $i$, for each of the three considered models.}
		\label{table:e-test-regression}
	\end{table}

	\subsection{Linear regression}

	\subsubsection{Aim}

	\todo{Explain what variable is predicted based on which other variables and what you hope to
		accomplish by the regression. Mention your feature transformation choices such as one-of-
		K coding. Since we will use regularization momentarily, apply a feature transformation to
		your data matrix X such that each column has mean 0 and standard deviation 1.}

	\todo{...}

	\subsubsection{Regularization}

	\todo{Introduce a regularization parameter $\lambda$ as discussed in 14 of the lecture notes, and estimate
		the generalization error for different values of $\lambda$. Specifically, choose a reasonable range of
		values of $\lambda$ (ideally one where the generalization error first drop and then increases), and
		for each value use $K = 10$ fold cross-validation (algorithm 5) to estimate the generalization
		error. Include a figure of the estimated generalization error as a function of $\lambda$ in the report
		and briefly discuss the result.}

	\todo{Figure}

	\todo{Reference figure}

	\todo{...}

	\subsubsection{Model interpretation}

	\todo{Explain how the output, $y$, of the linear model with the lowest generalization error (as
		determined in the previous question) is computed for a given input $x$. What is the effect
		of an individual attribute in $x$ on the output, $y$, of the linear model? Does the effect of
		individual attributes make sense based on your understanding of the problem?}

	\todo{Final equation}

	\todo{...}

	\subsection{Regularized linear regression vs an Artificial Neural Network}

	\todo{Rewrite this (concisely!) so that it isn't an exact copy of the assignment}

	In this section, we will compare three models: the regularized linear re-
	gression model from the previous section, an artificial neural network (ANN) and a baseline. We
	are interested in two questions: Is one model better than the other? Is either model better than
	a trivial baseline?. We will attempt to answer these questions with two-level cross-validation.

	\todo{Create the table as in the assignment (Vincent)}

	\todo{Write the accompanying text on how we retreived the data in the table.}

	\todo{Write out the statistical comparisons using data from the table.}

	\todo{TABLE: Include p-values and confidence intervals for the three pairwise tests in your report.}

	\todo{Conclude on the results from the values in the table and reference the table.}


	\section{Classification}

	\begin{table}
		\centering
		\renewcommand{\arraystretch}{1.2}
		\begin{tabular}{| r || l | l || l | l || l |}
			\hline
			\textbf{Outer fold} & \multicolumn{2}{c||}{\textbf{\todo{Decision Tree?}}} & \multicolumn{2}{c||}{\textbf{Logistic regression}} & \textbf{Baseline}     \\ \hline\hline
			\(i\)               & \(h_i^*\) & \(E_i^{\text{test}}\)                    & \(\lambda_i^*\)  & \(E_i^{\text{test}}\)           & \(E_i^{\text{test}}\) \\ \hline
			1                   & 10        & \SI{ 1.36364e-01 }{}                     & \SI{ 0.001 }{}   & \SI{ 2.27273e-01 }{}            & \SI{ 6.36364e-01 }{}  \\
			2                   & 6         & \SI{ 4.09091e-01 }{}                     & \SI{ 0.00886 }{} & \SI{ 2.72727e-01 }{}            & \SI{ 6.81818e-01 }{}  \\
			3                   & 4         & \SI{ 4.54545e-01 }{}                     & \SI{ 0.00886 }{} & \SI{ 4.09091e-01 }{}            & \SI{ 5.90909e-01 }{}  \\
			4                   & 4         & \SI{ 5.00000e-01 }{}                     & \SI{ 0.00886 }{} & \SI{ 4.54545e-01 }{}            & \SI{ 5.45455e-01 }{}  \\
			5                   & 7         & \SI{ 2.85714e-01 }{}                     & \SI{ 0.001 }{}   & \SI{ 3.33333e-01 }{}            & \SI{ 6.19048e-01 }{}  \\
			6                   & 10        & \SI{ 4.76190e-01 }{}                     & \SI{ 0.001 }{}   & \SI{ 5.71429e-01 }{}            & \SI{ 7.14286e-01 }{}  \\
			7                   & 8         & \SI{ 2.85714e-01 }{}                     & \SI{ 0.00886 }{} & \SI{ 3.33333e-01 }{}            & \SI{ 7.14286e-01 }{}  \\
			8                   & 5         & \SI{ 1.42857e-01 }{}                     & \SI{ 0.0183 }{}  & \SI{ 2.38095e-01 }{}            & \SI{ 6.19048e-01 }{}  \\
			9                   & 10        & \SI{ 4.76190e-01 }{}                     & \SI{ 0.001 }{}   & \SI{ 3.33333e-01 }{}            & \SI{ 8.57143e-01 }{}  \\
			10                  & 3         & \SI{ 4.28571e-01 }{}                     & \SI{ 0.0183 }{}  & \SI{ 4.76190e-01 }{}            & \SI{ 8.09524e-01 }{}  \\ \hline
		\end{tabular}
		\caption{Summary of two-level cross validation for predicting the glass type. Hyperparameters and test errors $E_i^\text{test}$ on $\mathcal{D}_i^\text{test}$ per outer fold $i$, for each of the three considered classificationmodels. \todo{Change $\lambda$ to $C$?}}
		\label{table:e-test-classification}
	\end{table}

	\todo{Choose method 2: ANN, CT, KNN, NB}

	\todo{Make visualisations of the results in \ref{table:e-test-classification}}

	\subsection{Introduction}

	\todo{Explain which classification problem you have chosen to solve. Is it a multi-class or binary
		classification problem?}

	\subsection{Logistic regression vs [...method 2...]}

	\todo{Rewrite the assignment below such that it (consisely!) states what we will do in this section.}

	We will compare logistic regression, method 2 and a baseline. For logistic regression, we
	will once more use $\lambda$ as a complexity-controlling parameter, and for method 2 a relevant
	complexity controlling parameter and range of values. We recommend this choice is made
	based on a trial run, which you do not need to report. Describe which parameter you have
	chosen and the possible values of the parameters you will examine. The baseline will be a
	model which compute the largest class on the training data, and predict everything in the
	test-data as belonging to that class (corresponding to the optimal prediction by a logistic
	regression model with a bias term and no features).

	\todo{Perform a statistical evaluation of your three models similar to the previous section. That
		is, compare the three models pairwise.}

	\todo{TABLE: Include p-values and confidence intervals for the three pairwise tests in your report.}

	\todo{Conclude on the results from the values in the table and reference the table.}

	\subsection{Interpretation of the LR model}

	\todo{Train a logistic regression model using a suitable value of $\lambda$ (see previous exercise). Explain
		how the logistic regression model make a prediction. Are the same features deemed relevant
		as for the regression part of the report?}


	\section*{Use of GenAI}

	...

	\bibliography{citations}
	\bibliographystyle{unsrt}

	\vspace*{1cm}
	\appendix

	\LARGE\bfseries Appendix

	\normalsize\normalfont

	\section{Repository and supplementary materials}
	The full notebook, scripts, and generated figures for this project are available in the project repository:
	\begin{quote}
	\url{https://github.com/schependom/DTU\_machine-learning-projects/tree/main}
	\end{quote}
	This repository contains the data-loading and analysis code that produced the tables and figures cited
	above (see the \texttt{figures/} folder for the PDF outputs referenced in the report).
%
%

%	\section{Test}

\end{document}
