\documentclass[dtu]{dtuarticle}
\usepackage{parskip} % use enters instead of indents

\newcommand{\todo}[1]{\color{red}[TODO: #1]\color{black}}
\newcommand*\chem[1]{\ensuremath{\mathrm{#1}}}
\usepackage{amsmath}
\usepackage{bm} % bold ITALIC math (for vectors!)
\usepackage{siunitx}
\sisetup{
	per-mode=fraction,
	detect-weight = true,
	detect-family = true,
	separate-uncertainty=true, % Dit is voor de plus-minus
	output-decimal-marker={.}
}

\usepackage{subcaption}

\title{Machine Learning Project 2}
\subtitle{Supervised Learning: Classification and Regression}
\author{Group 94}
\course{02452 Machine Learning}
\address{
	DTU Compute \\
	Fall 2025
}
\date{\today}


\begin{document}

	\maketitle

	\begin{table}[h!]
		\renewcommand{\arraystretch}{1.2}
		\centering
		\begin{subtable}{\textwidth}
			\centering
			\begin{tabular}{l | l}
				\textbf{Name}                 & \textbf{Student number} \\ \hline\hline
				Vincent Van Schependom        & s251739                 \\ \hline
				Diego Armando Mijares Ledezma & s251777                 \\ \hline
				Albert Joe Jensen             & s204601
			\end{tabular}
			\caption{Group members.}
			\label{table:members}
		\end{subtable}
		\\ \vspace*{0.5cm}
		\begin{subtable}{\textwidth}
			\centering
			\begin{tabular}{l | *{3}{|r}}
				\textbf{Task}                 & \textbf{Vincent} & \textbf{Diego} & \textbf{Albert} \\ \hline\hline
				Training \& test loops & 0\%              &            0\% &             0\% \\ \hline
				Coding visualisations         & 0\%              &            0\% &             0\% \\ \hline
				Section 3                     & 0\%              &            0\% &             0\% \\ \hline
				\LaTeX                        & 0\%              &            0\% &             0\%
			\end{tabular}
			\caption{Contributions \& responsabilities table.}
			\label{table:contributions}
		\end{subtable}
		\caption{Group information \& work distribution.}
	\end{table}

	\section*{Introduction}

	The objective of this report is to apply the methods that were discussed during the second
	section of the course \textit{Machine Learning} \cite{book} to a chosen dataset. The aim is to perform
	relevant regression and classification to the data.

	The particular dataset that is being investigated is -- just like in Project 1 -- the \textit{Glass Identification} dataset from 1987 by B. German \cite{dataset}. Table \ref{table:members} lists our full names and student numbers, while Table \ref{table:contributions} shows an overview of the contribution of each team member.

	\tableofcontents

	\newpage

	\section{Regression}

	We aim to predict the refractive index (\texttt{RI}) of glass samples from their chemical composition: the oxide
	components (\texttt{Na}, \texttt{Mg}, \texttt{Al}, \texttt{Si}, \texttt{K}, \texttt{Ca}, \texttt{Ba}, and \texttt{Fe}) and the categorical variable \texttt{Type}. Predicting \texttt{RI} is formulated
	as a regression problem where we evaluate the performance of a regularized linear model (Ridge Regression),
	an artificial neural network (ANN), and a trivial baseline predictor, which always predicts the mean \texttt{RI} of the training set.
	Numerical features are standardized, as argued previously in Report 1 \cite{report1}, and categorical variables are encoded using one-hot encoding.

	\todo{Add report 1 to the bibliography.}

	\subsection{Regularized linear regression}

	\subsubsection{Selecting the regularization parameter $\lambda$}

	To investigate the overfitting prevention effect of regularization, we evaluated Ridge Regression models
	across a wide range of regularization variable values from $\lambda = 10^{-2}$ to $10^{5}$ (in 50 logarithmic steps) using $K=10$-fold
	cross-validation. Figure~\ref{fig:rlr_mse_vs_lambda} shows a logarithmic plot of the average mean squared
	error (MSE) on the validation sets across all folds -- which is an estimate $\hat{E}^{\text{gen}}$ of the generalization error -- as a function of $\lambda$.

	The minimum average validation error was found at
	$\hat{\lambda} = \num{0.2683}$ with an average validation set MSE over all 10 of \num{1.003e-6}.
	We observe that the curve of MSE is relatively flat around the optimum, indicating
	a stable solution. As the regularization parameter $\lambda$ increases above $10$, and larger coefficients thus get more penalized, the model underfits, leading to a
	gradual rise in error to $\hat{E}^{\text{gen}} \approx \num{1e-5}$. Conversely, smaller values ($\lambda < 10^{-1}$) result in marginally higher average validation MSE due to overfitting.

	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.75\textwidth]{figures/rlr_mse_vs_lambda.pdf}
		\caption{Mean squared error (MSE) versus regularization strength $\lambda$
		for Ridge regression using nested cross-validation. The optimal $\lambda$
		($\hat{\lambda}=0.2683$) achieved a minimum average MSE of $1.0033\times10^{-6}$.}
		\label{fig:rlr_mse_vs_lambda}
	\end{figure}

	\todo{Create the figure that visualises the selected coefficients based on lambda (copy paste from latest exercise session and look at my two images in WhatsApp.)}

	\todo{Add these images here.}

	\todo{Add captions, reference from text and elaborate on observations.}




	\subsubsection{Model interpretation}

	A final Ridge Regression model was trained on the \textit{entire} dataset using the optimal
	regularization parameter $\hat{\lambda}=0.2683$. For a given input $\mathbf{x}$, the predicted refractive index $\hat{y} = \texttt{RI}$ is given by:

	\begin{align*}
		\texttt{RI} &= \beta_0   & + & \beta_1 \cdot \texttt{Na}           & + & \beta_2 \cdot \texttt{Mg}           & + & \beta_3 \cdot \texttt{Al}                 &  \\
		             &           & + & \beta_4 \cdot \texttt{Si}           & + & \beta_5 \cdot \texttt{K}            & + & \beta_6 \cdot \texttt{Ca}                 &  \\
		             &           & + & \beta_7 \cdot \texttt{Ba}           & + & \beta_8 \cdot \texttt{Fe}           & + & \beta_9 \cdot \texttt{BW-FP}              &  \\
		             &           & + & \beta_{10} \cdot \texttt{BW-NFP}    & + & \beta_{11} \cdot \texttt{VW-FP}     & + & \beta_{12} \cdot \texttt{containers}      &   \\
		             &           & + & \beta_{13} \cdot \texttt{headlamps} & + & \beta_{14} \cdot \texttt{tableware} & + & \beta_{15} \cdot \texttt{vehicle windows} & \\
					&= \num{000} & + & \num{000} \cdot \texttt{Na}           & + & \num{000} \cdot \texttt{Mg}           & + & \num{000} \cdot \texttt{Al}                 &  \\
		             &           & + & \num{000} \cdot \texttt{Si}           & + & \num{000} \cdot \texttt{K}            & + & \num{000} \cdot \texttt{Ca}                 &  \\
		             &           & + & \num{000} \cdot \texttt{Ba}           & + & \num{000} \cdot \texttt{Fe}           & + & \num{000} \cdot \texttt{BW-FP}              &  \\
		             &           & + & \num{000} \cdot \texttt{BW-NFP}    & + & \num{000} \cdot \texttt{VW-FP}     & + & \num{000} \cdot \texttt{containers}      &   \\
		             &           & + & \num{000} \cdot \texttt{headlamps} & + & \num{000} \cdot \texttt{tableware} & + & \num{000} \cdot \texttt{vehicle windows} &
	\end{align*}

	\todo{Fill in the coefficients above based on the Python output.}

	These coefficients reveal the relative contribution of each oxide concentration ($\beta_1$ to $\beta_8$) and glass type
	(one hot encoded $\beta_9$ to $\beta_{15}$) to the refractive index (\texttt{RI}).

	The elements \texttt{Ca} and \texttt{Ba} exhibit the
	largest positive coefficients, implying that increasing these oxides raises the estimated refractive index in this model.
	Conversely, \texttt{Si}, \texttt{Al}, and \texttt{Na} have negative coefficients,
	indicating that higher concentrations of these oxides decrease the estimated refractive index. The small magnitude of coefficients for
	\texttt{Mg} and \texttt{Fe} suggests a weaker influence on the estimated refractive index.

	\todo{Discuss the one-hot-encoded categorical variables here as well.}


	\subsubsection{Evaluation of the Ridge model}


	To verify the predictive performance of the final Ridge model,
	we evaluated it on the full dataset using the optimal $\hat{\lambda}$. This led to a mean squared error of $\text{MSE} = \num{8.5e-07}$ and a coefficient of determination of $R^2 = \num{0.91}$.

	Figure~\ref{fig:rlr_pred_vs_actual} shows predicted versus actual \texttt{RI} values.
	The points lie closely along the identity line, confirming accurate predictions.
	The residuals in Figure~\ref{fig:rlr_residuals} are randomly distributed around
	zero, suggesting no systematic bias or heteroscedasticity.

	\begin{figure}[ht]
	\centering
	\includegraphics[width=0.45\textwidth]{figures/rlr_predicted_vs_actual.pdf}
	\includegraphics[width=0.45\textwidth]{figures/rlr_residuals_vs_predicted.pdf}
	\caption{Verification of the regularized Ridge regression model.
	Left: predicted versus actual refractive index (RI).
	Right: residuals versus predicted RI. The model shows a high determination coefficient $R^2 = \num{0.91}$ and
	randomly distributed residuals, confirming stable behavior.}
	\label{fig:rlr_pred_vs_actual}
	\label{fig:rlr_residuals}
	\end{figure}




	\subsection{Comparison of regression models}

		\begin{table}
		\centering
		\renewcommand{\arraystretch}{1.2}
		\begin{tabular}{| r || l | l || l | l || l |}
			\hline
			\textbf{Outer fold} & \multicolumn{2}{c||}{\textbf{ANN}} & \multicolumn{2}{c||}{\textbf{Linear regression}} & \textbf{Baseline}     \\ \hline\hline
			\(i\)               & \(h_i^*\) & \(E_i^{\text{test}}\)  & \(\lambda_i^*\) & \(E_i^{\text{test}}\)          & \(E_i^{\text{test}}\) \\ \hline
1 & 500 & \SI{ 2.607e-02 }{} & \SI{ 0.797 }{} & \SI{ 1.600e-06 }{} & \SI{ 1.970e-05 }{} \\
2 & 480 & \SI{ 1.327e-02 }{} & \SI{ 2.84 }{} & \SI{ 4.249e-07 }{} & \SI{ 1.228e-05 }{} \\
3 & 230 & \SI{ 3.239e-02 }{} & \SI{ 0.325 }{} & \SI{ 4.821e-07 }{} & \SI{ 4.482e-06 }{} \\
4 & 190 & \SI{ 1.592e-02 }{} & \SI{ 0.167 }{} & \SI{ 9.633e-07 }{} & \SI{ 5.047e-06 }{} \\
5 & 120 & \SI{ 1.030e-02 }{} & \SI{ 1.43 }{} & \SI{ 1.500e-06 }{} & \SI{ 3.080e-06 }{} \\
6 & 500 & \SI{ 3.226e-03 }{} & \SI{ 0.01 }{} & \SI{ 2.142e-06 }{} & \SI{ 5.982e-06 }{} \\
7 & 120 & \SI{ 1.603e-02 }{} & \SI{ 0.325 }{} & \SI{ 1.284e-06 }{} & \SI{ 8.186e-06 }{} \\
8 & 240 & \SI{ 3.639e-02 }{} & \SI{ 0.01 }{} & \SI{ 1.018e-06 }{} & \SI{ 7.210e-06 }{} \\
9 & 290 & \SI{ 6.221e-02 }{} & \SI{ 1.27 }{} & \SI{ 1.637e-07 }{} & \SI{ 1.946e-05 }{} \\
10 & 500 & \SI{ 2.641e-02 }{} & \SI{ 3 }{} & \SI{ 5.225e-07 }{} & \SI{ 7.245e-06 }{} \\
			\hline
		\end{tabular}
		\caption{Summary of two-level cross validation for predicting the refractive index \texttt{RI} based on the chemical composition. Hyperparameters and test errors $E_i^\text{test}$ on $\mathcal{D}_i^\text{test}$ per outer fold $i$, for each of the three considered models.}
		\label{table:e-test-regression}
	\end{table}

	\begin{figure}
		\centering
		\includegraphics[width=0.75\textwidth]{figures/regression_test_errors_by_fold.pdf}
		\caption{Test errors $E_i^\text{test}$ across outer folds for each regression model.
			\todo{Elaborate on the observations in this caption.}, \todo{Reference this figure.}, \todo{Mention this is a semilog plot.}}
		\label{fig:regression_test_errors_by_fold}
	\end{figure}

	\subsubsection{Two-level cross-validation}

	This section compares regularized linear regression (from the previous section), an artificial neural network (ANN), and a baseline model. Two-level cross-validation is employed to determine relative performance of these models. As complexity-controlling parameter for the ANN, the number of hidden units $h$ is varied from $h=1$ to $h=990$ in steps of 10. The baseline models predicts the mean \texttt{RI} of the training set for all inputs.

	The nested cross-validation results for the regression tasks are summarized in Table~\ref{table:e-test-regression}. \todo{Elaborate on the results of the table!}

	\begin{figure}
		\centering
		\includegraphics[width=0.9\textwidth]{figures/ann_gen_error_vs_hidden_units.pdf}
		\caption{Mean squared error (MSE) on the validation set versus number of hidden units $h$
		for the artificial neural network using nested cross-validation.
		The optimal $h$ ($\hat{h}=120$) achieved a minimum average MSE of \num{1.03e-2}. \todo{Reference this figure!}}
		\label{fig:ann_mse_vs_h}
	\end{figure}

	\subsubsection{Statistical significance testing}

	Paired $t$-tests were conducted across folds to assess whether performance differences were statistically significant. The results are summarized in Table~\ref{tab:reg_pairwise}.
	All $p$-values below 0.05 indicate statistically significant differences.

	\begin{table}
		\centering
		\renewcommand{\arraystretch}{1.2}
		\begin{tabular}{ l || r | r | r | r | r | r}
			\textbf{Model comparison} & \textbf{$\hat{\mu}$} & \textbf{$\hat{\sigma}$} & \textbf{$t$-statistic} & \textbf{$p$-value} & \textbf{$t_{0.05}$} & \textbf{$t_{0.95}$} \\ \hline \hline
			ANN vs RLR                & \num{000}                & \num{000}            & \num{000}          & \num{000}           & \num{000}           & \num{000}           \\ \hline
			ANN vs Baseline           & \num{000}                & \num{000}            & \num{000}          & \num{000}           & \num{000}           & \num{000}           \\ \hline
			RLR vs Baseline           & \num{000}                & \num{000}            & \num{000}          & \num{000}           & \num{000}           & \num{000}           \\
		\end{tabular}
		\caption{Pairwise comparison of regression models (ANN = Artificial Neural Network, RLR = Regularized Linear Regression) using paired $t$-tests across outer folds.
		Mean differences $\hat{\mu}$ and standard deviations $\hat{\sigma}$ in test error $E^{\text{test}}$, $t$-statistics, and $p$-values are reported, as well as a 95\% confidence interval $[t_{0.05}, t_{0.95}]$.
		All comparisons show statistically significant differences ($p<0.05$). \todo{Fill in the numbers based on the Python output.}}
		\label{tab:reg_pairwise}
	\end{table}

	\todo{Check if we need to elaborate further / if this text contains mistakes:}

	\begin{itemize}

		\item \textbf{ANN vs RLR:} The ANN achieved a lower average test error, with a mean
		difference of $0.0242 \pm 0.0169$ ($t=4.54$, $p=0.0014$). This indicates the ANN
		significantly outperforms Ridge regression on the RI prediction task.

		\item \textbf{ANN vs Baseline:} The ANN also significantly outperformed the baseline
		($t=4.54$, $p=0.0014$), confirming that it captures non-linearities missed by
		linear models.

		\item \textbf{RLR vs Baseline:} Ridge regression slightly outperformed the baseline
		(mean difference $-8.26\times10^{-6}$, $t=-4.25$, $p=0.0021$), confirming that even
		a regularized linear model provides small but significant improvements over the
		naive predictor.

	\end{itemize}

	The ANN's advantage is thus both statistically and practically significant,
	while the Ridge regression, although simpler, remains interpretable and competitive.

	\section{Classification}

	\begin{table}
		\centering
		\renewcommand{\arraystretch}{1.2}
		\begin{tabular}{| r || l | l || l | l || l |}
			\hline
			\textbf{Outer fold} & \multicolumn{2}{c||}{\textbf{Decision Tree}} & \multicolumn{2}{c||}{\textbf{Logistic regression}} & \textbf{Baseline}     \\ \hline\hline
			\(i\)               & \(h_i^*\) & \(E_i^{\text{test}}\)            & \(\lambda_i^*\)  & \(E_i^{\text{test}}\)           & \(E_i^{\text{test}}\) \\ \hline
			1                   & 3         & \SI{ 0.4091 }{}                  & \SI{ 0.336 }{}   & \SI{ 0.2273 }{}                 & \SI{ 0.5455 }{}       \\
			2                   & 6         & \SI{ 0.2273 }{}                  & \SI{ 0.0785 }{}  & \SI{ 0.3182 }{}                 & \SI{ 0.5000 }{}       \\
			3                   & 15        & \SI{ 0.3182 }{}                  & \SI{ 0.00886 }{} & \SI{ 0.4091 }{}                 & \SI{ 0.8182 }{}       \\
			4                   & 3         & \SI{ 0.3636 }{}                  & \SI{ 0.0379 }{}  & \SI{ 0.4545 }{}                 & \SI{ 0.9091 }{}       \\
			5                   & 5         & \SI{ 0.3333 }{}                  & \SI{ 0.00886 }{} & \SI{ 0.2857 }{}                 & \SI{ 0.5714 }{}       \\
			6                   & 6         & \SI{ 0.4762 }{}                  & \SI{ 0.00428 }{} & \SI{ 0.4286 }{}                 & \SI{ 0.8095 }{}       \\
			7                   & 5         & \SI{ 0.2857 }{}                  & \SI{ 0.00428 }{} & \SI{ 0.3333 }{}                 & \SI{ 0.6667 }{}       \\
			8                   & 10        & \SI{ 0.2857 }{}                  & \SI{ 0.336 }{}   & \SI{ 0.3333 }{}                 & \SI{ 0.7619 }{}       \\
			9                   & 4         & \SI{ 0.3333 }{}                  & \SI{ 0.00428 }{} & \SI{ 0.3810 }{}                 & \SI{ 0.6667 }{}       \\
			10                  & 7         & \SI{ 0.3333 }{}                  & \SI{ 0.695 }{}   & \SI{ 0.4762 }{}                 & \SI{ 0.8095 }{}       \\ \hline
		\end{tabular}
		\caption{Summary of two-level cross validation for predicting the glass type. Hyperparameters and test errors $E_i^\text{test}$ on $\mathcal{D}_i^\text{test}$ per outer fold $i$, for each of the three considered classificationmodels. \todo{Change $\lambda$ to $C$?}}
		\label{table:e-test-classification}
	\end{table}

	\subsection{Introduction}

	We aim to predict the type of glass (\texttt{Type}), which can take on 7 values, of which only 6 are present in the training set, since there are no observations with \texttt{Type} = \texttt{VW-NFP}. We will use the oxide concentrations as
	predictors to handle this \textit{multi-class classification problem}. We compare Logistic Regression (LR), a Classification Tree (Decision Tree, DT), and a trivial baseline that always predicts the most frequent
	class present in the training set. All models were evaluated with nested (two-level) cross-validation and then statistically compared using paired $t$-tests across outer folds.

	\subsection{Comparison of classification models}

	\begin{figure}
		\centering
		\includegraphics[width=0.75\textwidth]{figures/classification_test_errors_by_fold.pdf}
		\caption{Test errors $E_i^\text{test}$ across outer folds for each classification model.
		Both the Logistic Regression and Decision Tree outperform the baseline in all folds,
		with Logistic Regression achieving slightly lower error on average., \todo{Mention this is a semilog plot.}}
		\label{fig:classification_test_errors_by_fold}
	\end{figure}

	This section compares Logistic Regression and a Decision Tree classifier against the baseline.
	For each outer fold, we optimized the regularization strength $C = 1/\lambda$ for Logistic Regression
	and the maximum tree depth $h$ for the Decision Tree using inner cross-validation.
	We then computed the test error $E_i^\text{test}$ on each outer test fold.

	\todo{Mention the results in the table, reference the table using the following command and elaborate on them!}:
	\verb*|\ref{table:e-test-classification}|

	As shown in Table~\ref{table:e-test-classification}, ... \todo{Fill in based on the results!}

	\subsection{Statistical significance testing}

	Paired $t$-tests across outer folds were used to assess
	statistical significance between the three classification models. The results are summarized in
	Table~\ref{tab:cls_pairwise}.

	\todo{Fill in the table below based on the Python output.}

	\begin{table}
		\centering
		\renewcommand{\arraystretch}{1.2}
		\begin{tabular}{ l || r | r | r | r | r | r}
			\textbf{Model comparison} & \textbf{$\hat{\mu}$} & \textbf{$\hat{\sigma}$} & \textbf{$t$-statistic} & \textbf{$p$-value} & \textbf{$t_{0.05}$} & \textbf{$t_{0.95}$} \\ \hline \hline
			DT vs LR               & \num{000}                & \num{000}            & \num{000}          & \num{000}           & \num{000}           & \num{000}           \\ \hline
			DT vs Baseline           & \num{000}                & \num{000}            & \num{000}          & \num{000}           & \num{000}           & \num{000}           \\ \hline
			LR vs Baseline           & \num{000}                & \num{000}            & \num{000}          & \num{000}           & \num{000}           & \num{000}           \\
		\end{tabular}
		\caption{Pairwise comparison of classification models (DT = Decision Tree, LR = Logistic Regression) using paired $t$-tests across outer folds.
		Mean differences $\hat{\mu}$ and standard deviations $\hat{\sigma}$ in test error $E^{\text{test}}$, $t$-statistics, and $p$-values are reported, as well as a 95\% confidence interval $[t_{0.05}, t_{0.95}]$.
		All comparisons show statistically significant differences ($p<0.05$). \todo{Fill in the numbers based on the Python output.}}
		\label{tab:cls_pairwise}
	\end{table}

	We summarize the findings of these statistical tests below:

	\todo{Check if we need to elaborate further / if this text contains mistakes. I don't know why, but Diego (or his AI) wrote two versions of this... So read both of them and pick the best one / merge them.}

	\begin{itemize}

		\item \textbf{Tree vs LogReg:} The difference in accuracy ($-0.0281$) is not statistically
		significant ($p=0.375$), suggesting both models perform similarly.

		\item \textbf{Tree vs Baseline:} The decision tree significantly outperformed
		the baseline ($t=-8.91$, $p<10^{-5}$), indicating it captures meaningful
		structure in the data.

		\item \textbf{LogReg vs Baseline:} Logistic regression also significantly
		outperformed the baseline ($t=-13.34$, $p=3.1\times10^{-7}$), showing that even
		a simple linear classifier captures predictive signal.

	\end{itemize}

	\begin{itemize}

		\item \textbf{Tree vs LogReg:} The mean difference in error was $-0.0281 \pm 0.0953$
		($t=-0.93$, $p=0.375$), indicating no statistically significant difference. Both models perform comparably.

		\item \textbf{Tree vs Baseline:} The Decision Tree significantly outperformed the baseline
		with $t=-8.91$ and $p=9.25\times10^{-6}$, showing that even a simple tree captures
		predictive relationships among the oxide features.

		\item \textbf{LogReg vs Baseline:} Logistic Regression also achieved a strong improvement
		over the baseline ($t=-13.34$, $p=3.12\times10^{-7}$), confirming that linear decision
		boundaries explain most of the separability in the dataset.

	\end{itemize}

	In summary, both classification models substantially improve upon the baseline,
	and their similar performance suggests that the data can be effectively separated
	by relatively simple decision boundaries.

	Figure~\ref{fig:classification_test_errors_by_fold} visualizes the outer test errors per fold for
	the three models, clearly showing that both Logistic Regression and the Decision Tree
	consistently outperform the baseline classifier.


	\subsection{Interpretation of the LR model}

	A final Logistic Regression model was trained on the \textit{full} dataset using the selected regularization
	parameter $C^* = 1 / \hat{\lambda}$ from cross-validation. The model assigns a weight vector $\bm{w}_k$ to each class $k$, such that class scores are
	\[
	s_k(\bm{x}) = w_{k,0} + \bm{w}_k^\top \bm{x},
	\]
	and predicted probabilities are given by the softmax transformation.

	\todo{Check this part below for mistakes / elaborate further if needed:}

	The sign and magnitude of each coefficient indicate how increasing a given oxide concentration
	affects the log-odds of a sample belonging to class $k$. Features such as
	\texttt{Si}, \texttt{Ca}, and \texttt{Ba} were among the strongest predictors, consistent with the
	results from the regression analysis, where these elements also exhibited large coefficients
	in determining refractive index. This agreement suggests that the same chemical
	components governing RI also drive differences between glass types.

	Overall, Logistic Regression achieves competitive classification accuracy with interpretable
	coefficients, while the Decision Tree offers slightly lower performance but more explicit
	decision boundaries.


	\section*{Use of GenAI}

	Generative AI (ChatGPT by OpenAI and Claude by Anthropic) was used as a support tool during the project.

	Its role was limited to assisting with code debugging, identifying and fixing minor
	implementation errors, and -- for Diego -- writing drafts for parts of the report -- which were later
	heavily revised to form a cohesive final report.

	All analytical choices, data processing steps, and result interpretations were made
	by the project members \textit{themselves} based on the actual model outputs and \textit{their} understanding of the
	underlying methods.


	\bibliography{citations}
	\bibliographystyle{unsrt}

	\vspace*{1cm}
	\appendix

	\LARGE\bfseries Appendix

	\normalsize\normalfont

	\section{Repository and supplementary materials}
	The full notebook, scripts, and generated figures for this project are available in the project repository:
	\begin{quote}
	\url{https://github.com/schependom/DTU\_machine-learning-projects/tree/main}
	\end{quote}
	This repository contains the data-loading and analysis code that produced the tables and figures cited
	above (see the \texttt{figures/} folder for the PDF outputs referenced in the report).


\end{document}
