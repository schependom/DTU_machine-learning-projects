\documentclass[dtu]{dtuarticle}
\usepackage{parskip} % use enters instead of indents

\newcommand{\todo}[1]{\color{red}[TODO: #1]\color{black}}
\newcommand*\chem[1]{\ensuremath{\mathrm{#1}}}
\usepackage{amsmath}
\usepackage{bm} % bold ITALIC math (for vectors!)
\usepackage{siunitx}
\sisetup{
	per-mode=fraction,
	detect-weight = true,
	detect-family = true,
	separate-uncertainty=true, % Dit is voor de plus-minus
	output-decimal-marker={.}
}

\usepackage{subcaption}

\title{Machine Learning Project 2}
\subtitle{Supervised Learning: Classification and Regression}
\author{Group 94}
\course{02452 Machine Learning}
\address{
	DTU Compute \\
	Fall 2025
}
\date{\today}


\begin{document}

	\maketitle

	\begin{table}[h!]
		\renewcommand{\arraystretch}{1.2}
		\centering
		\begin{subtable}{\textwidth}
			\centering
			\begin{tabular}{l | l}
				\textbf{Name}                 & \textbf{Student number} \\ \hline\hline
				Vincent Van Schependom        & s251739                 \\ \hline
				Diego Armando Mijares Ledezma & s251777                 \\ \hline
				Albert Joe Jensen             & s204601
			\end{tabular}
			\caption{Group members.}
			\label{table:members}
		\end{subtable}
		\\ \vspace*{0.5cm}
		\begin{subtable}{\textwidth}
			\centering
			\begin{tabular}{l | *{3}{|r}}
				\textbf{Task}                 & \textbf{Vincent} & \textbf{Diego} & \textbf{Albert} \\ \hline\hline
				Training \& test loops & 0\%              &            0\% &             0\% \\ \hline
				Coding visualisations         & 0\%              &            0\% &             0\% \\ \hline
				Section 3                     & 0\%              &            0\% &             0\% \\ \hline
				\LaTeX                        & 0\%              &            0\% &             0\%
			\end{tabular}
			\caption{Contributions \& responsabilities table.}
			\label{table:contributions}
		\end{subtable}
		\caption{Group information \& work distribution.}
	\end{table}

	\section*{Introduction}

	The objective of this report is to apply the methods that were discussed during the second
	section of the course \textit{Machine Learning} \cite{book} to a chosen dataset. The aim is to perform
	relevant regression and classification to the data.

	The particular dataset that is being investigated is -- just like in Project 1 -- the \textit{Glass Identification} dataset from 1987 by B. German \cite{dataset}. Table \ref{table:members} lists our full names and student numbers, while Table \ref{table:contributions} shows an overview of the contribution of each team member.

	\tableofcontents

	\newpage

	\section{Regression}

	We aim to predict the refractive index (\texttt{RI}) of glass samples from their chemical composition: the oxide
	components (\texttt{Na}, \texttt{Mg}, \texttt{Al}, \texttt{Si}, \texttt{K}, \texttt{Ca}, \texttt{Ba}, and \texttt{Fe}) and the categorical variable \texttt{Type}. Predicting \texttt{RI} is formulated
	as a regression problem where we evaluate the performance of a regularized linear model (Ridge Regression),
	an artificial neural network (ANN), and a trivial baseline predictor, which always predicts the mean \texttt{RI} of the training set.
	Numerical features are standardized, as argued previously in Report 1 \cite{report1}, and categorical variables are encoded using one-hot encoding.

	\todo{Add report 1 to the bibliography.}

	\subsection{Regularized linear regression}

	\subsubsection{Selecting the regularization parameter $\lambda$}

	To investigate the overfitting prevention effect of regularization, we evaluated Ridge Regression models
	across a wide range of regularization variable values from $\lambda = 10^{-2}$ to $10^{5}$ (in 50 logarithmic steps) using $K=10$-fold
	cross-validation. Figure~\ref{fig:rlr_mse_vs_lambda} shows a logarithmic plot of the average mean squared
	error (MSE) on the validation sets across all folds -- which is an estimate $\hat{E}^{\text{gen}}$ of the generalization error -- as a function of $\lambda$.

	The minimum average validation error was found at
	$\hat{\lambda} = \num{0.2683}$ with an average validation set MSE over all 10 of \num{1.003e-6}.
	We observe that the curve of MSE is relatively flat around the optimum, indicating
	a stable solution. As the regularization parameter $\lambda$ increases above $10$, and larger coefficients thus get more penalized, the model underfits, leading to a
	gradual rise in error to $\hat{E}^{\text{gen}} \approx \num{1e-5}$. Conversely, smaller values ($\lambda < 10^{-1}$) result in marginally higher average validation MSE due to overfitting.

	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.75\textwidth]{Figures/rlr_mse_vs_lambda.pdf}
		\caption{Mean squared error (MSE) versus regularization strength $\lambda$
		for Ridge regression using nested cross-validation. The optimal $\lambda$
		($\hat{\lambda}=0.2683$) achieved a minimum average MSE of $1.0033\times10^{-6}$.}
		\label{fig:rlr_mse_vs_lambda}
	\end{figure}

	\todo{Create the figure that visualises the selected coefficients based on lambda (copy paste from latest exercise session and look at my two images in WhatsApp.)}
	\todo{Add these images here.}
	\todo{Add captions, reference from text and elaborate on observations.}

	Following this, we visualize how the Ridge regression coefficients and the model errors evolve as a function of the regularization strength $\lambda$, 
	as shown in Figure~\ref{fig:coefficients_and_errors_vs_lambda}.

	
	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.48\textwidth]{Figures/coefficients_and_errors_vs_lambda.png}
		\caption{Left: Evolution of Ridge regression coefficients as a function of the regularization strength $\lambda$. 
		Right: Mean squared error (MSE) for training and test sets averaged over cross-validation folds, with the optimal $\lambda$ indicated by a yellow star.}
		\label{fig:coefficients_and_errors_vs_lambda}
	\end{figure}

	In the left panel of Figure~\ref{fig:coefficients_and_errors_vs_lambda}, we observe that as $\lambda$ increases (stronger regularization), 
	the magnitude of all coefficients shrinks toward zero, indicating the penalization effect of the Ridge term.

	In the right panel, the mean squared error (MSE) for both the training and test sets is plotted as a function of $\lambda$. For low regularization strengths, 
	the training error remains small while the test error is higher, which may indicate some overfitting. As $\lambda$ increases, both errors rise, but the test error 
	initially very slightly decreases (not visible to the eye with the graph due to scale), until reaching a minimum at the optimal value $\lambda^* \approx 0.27$, 
	before increasing, due to possibly underfitting. This demonstrates the trade-off between model complexity and regularization strength, with the selected $\lambda^*$ 
	providing the best generalization performance.

	\subsubsection{Model interpretation}

	A final Ridge Regression model was trained on the \textit{entire} dataset using the optimal
	regularization parameter $\hat{\lambda}=0.2683$. For a given input $\mathbf{x}$, the predicted refractive index $\hat{y} = \texttt{RI}$ is given by:

	\begin{align*}
		\texttt{RI} &= \beta_0   & + & \beta_1 \cdot \texttt{Na}           & + & \beta_2 \cdot \texttt{Mg}           & + & \beta_3 \cdot \texttt{Al}                 &  \\
		             &           & + & \beta_4 \cdot \texttt{Si}           & + & \beta_5 \cdot \texttt{K}            & + & \beta_6 \cdot \texttt{Ca}                 &  \\
		             &           & + & \beta_7 \cdot \texttt{Ba}           & + & \beta_8 \cdot \texttt{Fe}           & + & \beta_9 \cdot \texttt{BW-FP}              &  \\
		             &           & + & \beta_{10} \cdot \texttt{BW-NFP}    & + & \beta_{11} \cdot \texttt{VW-FP}     & + & \beta_{12} \cdot \texttt{containers}      &   \\
		             &           & + & \beta_{13} \cdot \texttt{headlamps} & + & \beta_{14} \cdot \texttt{tableware} & + & \beta_{15} \cdot \texttt{vehicle windows} & \\
					&= \num{1.518404089} & + & \num{3.174965268e-05} \cdot \texttt{Na}           & + & \num{0.001039499948} \cdot \texttt{Mg}           & + & \num{-0.0007494173277} \cdot \texttt{Al}                 &  \\
		             &           & + & \num{-0.0009319408212} \cdot \texttt{Si}           & + & \num{9.353293019e-05} \cdot \texttt{K}            & + & \num{0.002675716839} \cdot \texttt{Ca}                 &  \\
		             &           & + & \num{0.0006902870123} \cdot \texttt{Ba}           & + & \num{1.500057334e-05} \cdot \texttt{Fe}           & + & \num{-0.0002326391746} \cdot \texttt{BW-FP}              &  \\
		             &           & + & \num{-0.0001751580864} \cdot \texttt{BW-NFP}    & + & \num{-0.00113567091} \cdot \texttt{VW-FP}     & + & \num{0.000109708116} \cdot \texttt{containers}      &   \\
		             &           & + & \num{0.001314906053} \cdot \texttt{headlamps} & + & \num{0.0001188540026} \cdot \texttt{tableware} & 
	\end{align*}

	\todo{Fill in }

	These coefficients reveal the relative contribution of each oxide concentration ($\beta_1$ to $\beta_8$) and glass type
	(one hot encoded $\beta_9$ to $\beta_{15}$) to the refractive index (\texttt{RI}). 

	The elements \texttt{Ca} and \texttt{Ba} exhibit the
	largest positive coefficients, implying that increasing these oxides raises the estimated refractive index in this model.
	Conversely, \texttt{Si}, \texttt{Al}, and \texttt{Na} have negative coefficients,
	indicating that higher concentrations of these oxides decrease the estimated refractive index. The small magnitude of coefficients for
	\texttt{Mg} and \texttt{Fe} suggests a weaker influence on the estimated refractive index.

	\todo{Discuss the one-hot-encoded categorical variables here as well.}

	Most of these categorical coefficients are very small in magnitude, indicating that the glass type has a relatively minor effect 
	on the refractive index compared to the chemical composition.  \texttt{VW-FP} has a negative coefficient, 
	suggesting that this type slightly decreases the predicted \texttt{RI}.  

	\texttt{vehicle windows} has a coefficient of 0. This does not mean it has no effect in reality; rather, there were too few observations for the model to estimate its effect. 
	In this model, while the categorical glass types are included, the chemical composition dominates the predictions of \texttt{RI}.


	\subsubsection{Evaluation of the Ridge model}


	To verify the predictive performance of the final Ridge model,
	we evaluated it on the full dataset using the optimal $\hat{\lambda}$. This led to a mean squared error of $\text{MSE} = \num{8.5e-07}$ and a coefficient of determination of $R^2 = \num{0.91}$.

	Figure~\ref{fig:rlr_pred_vs_actual} shows predicted versus actual \texttt{RI} values.
	The points lie closely along the identity line, confirming accurate predictions.
	The residuals in Figure~\ref{fig:rlr_residuals} are randomly distributed around
	zero, suggesting no systematic bias or heteroscedasticity.

	\begin{figure}[ht]
	\centering
	\includegraphics[width=0.45\textwidth]{figures/rlr_predicted_vs_actual.pdf}
	\includegraphics[width=0.45\textwidth]{figures/rlr_residuals_vs_predicted.pdf}
	\caption{Verification of the regularized Ridge regression model.
	Left: predicted versus actual refractive index (RI).
	Right: residuals versus predicted RI. The model shows a high determination coefficient $R^2 = \num{0.91}$ and
	randomly distributed residuals, confirming stable behavior.}
	\label{fig:rlr_pred_vs_actual}
	\label{fig:rlr_residuals}
	\end{figure}




	\subsection{Comparison of regression models}

		\begin{table}
		\centering
		\renewcommand{\arraystretch}{1.2}
		\begin{tabular}{| r || l | l || l | l || l |}
			\hline
			\textbf{Outer fold} & \multicolumn{2}{c||}{\textbf{ANN}} & \multicolumn{2}{c||}{\textbf{Linear regression}} & \textbf{Baseline}     \\ \hline\hline
			\(i\)               & \(h_i^*\) & \(E_i^{\text{test}}\)  & \(\C_i^*\) & \(E_i^{\text{test}}\)          & \(E_i^{\text{test}}\) \\ \hline
1 & 500 & \SI{ 2.607e-02 }{} & \SI{ 0.797 }{} & \SI{ 1.600e-06 }{} & \SI{ 1.970e-05 }{} \\
2 & 480 & \SI{ 1.327e-02 }{} & \SI{ 2.84 }{} & \SI{ 4.249e-07 }{} & \SI{ 1.228e-05 }{} \\
3 & 230 & \SI{ 3.239e-02 }{} & \SI{ 0.325 }{} & \SI{ 4.821e-07 }{} & \SI{ 4.482e-06 }{} \\
4 & 190 & \SI{ 1.592e-02 }{} & \SI{ 0.167 }{} & \SI{ 9.633e-07 }{} & \SI{ 5.047e-06 }{} \\
5 & 120 & \SI{ 1.030e-02 }{} & \SI{ 1.43 }{} & \SI{ 1.500e-06 }{} & \SI{ 3.080e-06 }{} \\
6 & 500 & \SI{ 3.226e-03 }{} & \SI{ 0.01 }{} & \SI{ 2.142e-06 }{} & \SI{ 5.982e-06 }{} \\
7 & 120 & \SI{ 1.603e-02 }{} & \SI{ 0.325 }{} & \SI{ 1.284e-06 }{} & \SI{ 8.186e-06 }{} \\
8 & 240 & \SI{ 3.639e-02 }{} & \SI{ 0.01 }{} & \SI{ 1.018e-06 }{} & \SI{ 7.210e-06 }{} \\
9 & 290 & \SI{ 6.221e-02 }{} & \SI{ 1.27 }{} & \SI{ 1.637e-07 }{} & \SI{ 1.946e-05 }{} \\
10 & 500 & \SI{ 2.641e-02 }{} & \SI{ 3 }{} & \SI{ 5.225e-07 }{} & \SI{ 7.245e-06 }{} \\
			\hline
		\end{tabular}
		\caption{Summary of two-level cross validation for predicting the refractive index \texttt{RI} based on the chemical composition. Hyperparameters and test errors $E_i^\text{test}$ on $\mathcal{D}_i^\text{test}$ per outer fold $i$, for each of the three considered models.}
		\label{table:e-test-regression}
	\end{table}

	\begin{figure}
		\centering
		\includegraphics[width=0.75\textwidth]{figures/regression_test_errors_by_fold.pdf}
		\caption{Test errors $E_i^\text{test}$ across outer folds for each regression model.
			\todo{Elaborate on the observations in this caption.}, \todo{Reference this figure.}, \todo{Mention this is a semilog plot.}}
		\label{fig:regression_test_errors_by_fold}
	\end{figure}

	\subsubsection{Two-level cross-validation}

	This section compares regularized linear regression (from the previous section), an artificial neural network (ANN), and a baseline model. Two-level cross-validation is employed to determine relative performance of these models. As complexity-controlling parameter for the ANN, the number of hidden units $h$ is varied from $h=1$ to $h=990$ in steps of 10. The baseline models predicts the mean \texttt{RI} of the training set for all inputs.

	The nested cross-validation results for the regression tasks are summarized in Table~\ref{table:e-test-regression}. \todo{Elaborate on the results of the table!}

	The nested cross-validation results show that the Linear Regression model consistently achieves extremely low test errors
	across all folds, demonstrating excellent generalization and stable performance. In contrast, the ANN consistently produces higher 
	test errors (0.003 to 0.062) across all folds, indicating poor generalization and sensitivity to hyperparameter choice. The baseline 
	predictor performs better than the ANN but worse than Linear Regression, confirming that while a simple model captures some structure in the data, 
	Linear Regression clearly provides the most accurate and reliable predictions. The test errors are plotted in Figure~\ref{fig:regression_test_errors_by_fold}, 
	illustrating the consistent superiority of Linear Regression over both the ANN and baseline models.

	Figure~\ref{fig:ann_mse_vs_h} shows the mean squared error (MSE) on the validation set versus the number of hidden units. 
	We observe that the optimal $h$ ($\hat{h}=120$) achieved the minimum average MSE of \num{1.03e-2}, indicating that a moderately sized hidden layer performs best (we can say 120 is moderate out of 960 because it is substantial but not big in the context).

	\begin{figure}
		\centering
		\includegraphics[width=0.9\textwidth]{figures/ann_gen_error_vs_hidden_units.pdf}
		\caption{Mean squared error (MSE) on the validation set versus number of hidden units $h$
		for the artificial neural network using nested cross-validation.
		The optimal $h$ ($\hat{h}=120$) achieved a minimum average MSE of \num{1.03e-2}. \todo{Reference this figure!}}
		\label{fig:ann_mse_vs_h}
	\end{figure}

	\subsubsection{Statistical significance testing}

	Paired $t$-tests were conducted across folds to assess whether performance differences were statistically significant, following Setup I (11.3) 
	where the training set is fixed and only fold-wise variability is considered. The results are summarized in Table~\ref{tab:reg_pairwise}. 
	All $p$-values below 0.05 indicate statistically significant differences.


	\begin{table}
		\centering
		\renewcommand{\arraystretch}{1.2}
		\begin{tabular}{ l || r | r | r | r | r | r}
			\textbf{Model comparison} & \textbf{$\hat{\mu}$} & \textbf{$\hat{\sigma}$} & \textbf{$t$-statistic} & \textbf{$p$-value} & \textbf{$t_{0.05}$} & \textbf{$t_{0.95}$} \\ \hline \hline
			ANN vs RLR                & \num{0.024220412}                & \num{0.016855161}            & \num{4.54411}          & \num{0.00139767}           & \num{0.012162957}           & \num{0.036277868}           \\ \hline
			ANN vs Baseline           & \num{0.024212154}                & \num{0.016851428}            & \num{4.54356}          & \num{0.00139876}           & \num{0.012157369}           & \num{0.03626694}           \\ \hline
			RLR vs Baseline           & \num{-8.2580123e-06}                & \num{6.145326e-06}            & \num{-4.24943}          & \num{0.00214427}           & \num{-1.2654114e-05}           & \num{-3.8619109e-06}           \\
		\end{tabular}
		\caption{Pairwise comparison of regression models (ANN = Artificial Neural Network, RLR = Regularized Linear Regression) using paired $t$-tests across outer folds.
		Mean differences $\hat{\mu}$ and standard deviations $\hat{\sigma}$ in test error $E^{\text{test}}$, $t$-statistics, and $p$-values are reported, as well as a 95\% confidence interval $[t_{0.05}, t_{0.95}]$.
		All comparisons show statistically significant differences ($p<0.05$). \todo{Fill in the numbers based on the Python output.}}
		\label{tab:reg_pairwise}
	\end{table}

	\todo{Check if we need to elaborate further / if this text contains mistakes: it did}

	\begin{itemize}
    \item \textbf{ANN vs RLR:} The ANN consistently shows higher test errors than Ridge Linear Regression, 
	with a mean difference of about 0.024, indicating a statistically significant gap in performance.  
    \item \textbf{ANN vs Baseline:} The ANN also performs worse than the baseline predictor, with a similar mean difference, 
	confirming its instability and poorer generalization.  
    \item \textbf{RLR vs Baseline:} Ridge Linear Regression slightly outperforms the baseline, with a very small negative mean difference, 
	but the confidence interval excludes zero, demonstrating that even a simple regularized linear model reliably improves predictions over the baseline.
\end{itemize}

These results clearly support the ranking \textbf{ANN $<$ Baseline $<$ RLR}, and Table~\ref{tab:reg_pairwise} quantifies the magnitude and statistical significance of the differences between models.

	\section{Classification}

	\begin{table}
		\centering
		\renewcommand{\arraystretch}{1.2}
		\begin{tabular}{| r || l | l || l | l || l |}
			\hline
			\textbf{Outer fold} & \multicolumn{2}{c||}{\textbf{Decision Tree}} & \multicolumn{2}{c||}{\textbf{Logistic regression}} & \textbf{Baseline}     \\ \hline\hline
			\(i\)               & \(h_i^*\) & \(E_i^{\text{test}}\)            & \(\lambda_i^*\)  & \(E_i^{\text{test}}\)           & \(E_i^{\text{test}}\) \\ \hline
			1                   & 3         & \SI{ 0.4091 }{}                  & \SI{ 0.336 }{}   & \SI{ 0.2273 }{}                 & \SI{ 0.5455 }{}       \\
			2                   & 6         & \SI{ 0.2273 }{}                  & \SI{ 0.0785 }{}  & \SI{ 0.3182 }{}                 & \SI{ 0.5000 }{}       \\
			3                   & 15        & \SI{ 0.3182 }{}                  & \SI{ 0.00886 }{} & \SI{ 0.4091 }{}                 & \SI{ 0.8182 }{}       \\
			4                   & 3         & \SI{ 0.3636 }{}                  & \SI{ 0.0379 }{}  & \SI{ 0.4545 }{}                 & \SI{ 0.9091 }{}       \\
			5                   & 5         & \SI{ 0.3333 }{}                  & \SI{ 0.00886 }{} & \SI{ 0.2857 }{}                 & \SI{ 0.5714 }{}       \\
			6                   & 6         & \SI{ 0.4762 }{}                  & \SI{ 0.00428 }{} & \SI{ 0.4286 }{}                 & \SI{ 0.8095 }{}       \\
			7                   & 5         & \SI{ 0.2857 }{}                  & \SI{ 0.00428 }{} & \SI{ 0.3333 }{}                 & \SI{ 0.6667 }{}       \\
			8                   & 10        & \SI{ 0.2857 }{}                  & \SI{ 0.336 }{}   & \SI{ 0.3333 }{}                 & \SI{ 0.7619 }{}       \\
			9                   & 4         & \SI{ 0.3333 }{}                  & \SI{ 0.00428 }{} & \SI{ 0.3810 }{}                 & \SI{ 0.6667 }{}       \\
			10                  & 7         & \SI{ 0.3333 }{}                  & \SI{ 0.695 }{}   & \SI{ 0.4762 }{}                 & \SI{ 0.8095 }{}       \\ \hline
		\end{tabular}
		\caption{Summary of two-level cross validation for predicting the glass type. Hyperparameters and test errors $E_i^\text{test}$ on $\mathcal{D}_i^\text{test}$ per outer fold $i$, for each of the three considered classification
		 models. \todo{Change $\lambda$ to $C$? we didnt because we're unsure.}}
		\label{table:e-test-classification}
	\end{table}

	\subsection{Introduction}

	We aim to predict the type of glass (\texttt{Type}), which can take on 7 values, of which only 6 are present in the training set, since there are no observations with \texttt{Type} = \texttt{VW-NFP}. We will use the oxide concentrations as
	predictors to handle this \textit{multi-class classification problem}. We compare Logistic Regression (LR), a Classification Tree (Decision Tree, DT), and a trivial baseline that always predicts the most frequent
	class present in the training set. All models were evaluated with nested (two-level) cross-validation and then statistically compared using paired $t$-tests across outer folds.

	\subsection{Comparison of classification models}

	\begin{figure}
		\centering
		\includegraphics[width=0.75\textwidth]{figures/classification_test_errors_by_fold.pdf}
		\caption{Test errors $E_i^\text{test}$ across outer folds for each classification model.
		Both the Logistic Regression and Decision Tree outperform the baseline in all folds,
		with Logistic Regression achieving slightly lower error on average. It is important note that this is a semilogarithmic plot in which the y-axis only is on a logarithmic scale.}
		\label{fig:classification_test_errors_by_fold}
	\end{figure}

	To better visualize the wide range of test errors across models, the results are also presented in a semilogarithmic plot, 
	where the y-axis is on a logarithmic scale. This representation highlights the consistently low errors of Linear Regression 
	relative to the baseline and the higher errors of the ANN, making performance differences between models visually apparent.

	This section compares Logistic Regression and a Decision Tree classifier against the baseline.
	For each outer fold, we optimized the regularization strength $C = 1/\lambda$ for Logistic Regression
	and the maximum tree depth $h$ for the Decision Tree using inner cross-validation.
	We then computed the test error $E_i^\text{test}$ on each outer test fold.

	\todo{Mention the results in the table, reference the table using the following command and elaborate on them!}:
	\verb*|\ref{table:e-test-classification}|

	As shown in Table~\ref{table:e-test-classification}, the two-level cross-validation results reveal that the Decision Tree model 
	achieves the best test errors overall, outperforming the Logistic Regression in most cases, which can also be seen in Figure~\ref{fig:classification_test_errors_by_fold} 
	and also substantially surpassing the baseline. The Decision Tree is superior to 
	Logistic Regression in 7 out of 10 folds, suggesting that simple, non-overfitting structures suffice for this task. Logistic Regression hyperparameters $\lambda_i^*$ vary 
	widely (from \num{0.00428} to \SI{0.695}{}), reflecting data-dependent regularization needs. Nonetheless, it consistently beats the baseline 
	in all folds. Both models demonstrate robust improvements over the naive predictor, highlighting the value of even basic machine learning 
	approaches for glass type classification.

	\subsection{Statistical significance testing}

	Paired $t$-tests across outer folds were used to assess
	statistical significance between the three classification models. The statistical significance analysis corresponds to Setup II (Section 11.4), 
	as it evaluates model performance across multiple random training/test splits (outer cross-validation folds) using paired t-tests. 
	This setup accounts for the variability in model performance due to different training sets, unlike Setup I (McNemarâ€™s test), which applies 
	to a fixed train/test split. The results are summarized in
	Table~\ref{tab:cls_pairwise}.

	\todo{Fill in the table below based on the Python output.}

	\begin{table}
		\centering
		\renewcommand{\arraystretch}{1.2}
		\begin{tabular}{ l || r | r | r | r | r | r}
			\textbf{Model comparison} & \textbf{$\hat{\mu}$} & \textbf{$\hat{\sigma}$} & \textbf{$t$-statistic} & \textbf{$p$-value} & \textbf{$t_{0.05}$} & \textbf{$t_{0.95}$} \\ \hline \hline
			DT vs LR               & \num{-0.028138528}                & \num{0.095265421}            & \num{-0.934041}          & \num{0.374663}           & \num{-0.096287305}           & \num{0.040010249}           \\ \hline
			DT vs Baseline           & \num{-0.36926407}                & \num{0.13101627}            & \num{-8.91275}          & \num{9.24565e-06}           & \num{-0.46298746}           & \num{-0.27554068}           \\ \hline
			LR vs Baseline           & \num{-0.34112554}                & \num{0.080886635}            & \num{-13.3364}          & \num{3.11746e-07}           & \num{-0.39898835}           & \num{-0.28326273}           \\
		\end{tabular}
		\caption{Pairwise comparison of classification models (DT = Decision Tree, LR = Logistic Regression) using paired $t$-tests across outer folds.
		Mean differences $\hat{\mu}$ and standard deviations $\hat{\sigma}$ in test error $E^{\text{test}}$, $t$-statistics, and $p$-values are reported, as well as a 95\% confidence interval $[t_{0.05}, t_{0.95}]$.
		All comparisons show statistically significant differences ($p<0.05$). \todo{Fill in the numbers based on the Python output.}}
		\label{tab:cls_pairwise}
	\end{table}

	We summarize the findings of these statistical tests below:

	\todo{Check if we need to elaborate further / if this text contains mistakes. I don't know why, but Diego (or his AI) wrote two versions of this... So read both of them and pick the best one / merge them.}

	\begin{itemize}

		\item \textbf{Tree vs LogReg:} The difference in accuracy ($-0.0281$) is not statistically
		significant ($p=0.375$), suggesting both models perform similarly.

		\item \textbf{Tree vs Baseline:} The decision tree significantly outperformed
		the baseline ($t=-8.91$, $p<10^{-5}$), indicating it captures meaningful
		structure in the data.

		\item \textbf{LogReg vs Baseline:} Logistic regression also significantly
		outperformed the baseline ($t=-13.34$, $p=3.1\times10^{-7}$), showing that even
		a simple linear classifier captures predictive signal.

	\end{itemize}

	In summary, both classification models substantially improve upon the baseline,
	and their similar performance suggests that the data can be effectively separated
	by relatively simple decision boundaries.


	\subsection{Interpretation of the LR model}

	A final Logistic Regression model was trained on the \textit{full} dataset using the selected regularization
	parameter $C^* = 1 / \hat{\lambda}$ from cross-validation. The model assigns a weight vector $\bm{w}_k$ to each class $k$, such that class scores are
	\[
	s_k(\bm{x}) = w_{k,0} + \bm{w}_k^\top \bm{x},
	\]
	and predicted probabilities are given by the softmax transformation.

	\todo{I had to add code because I couldn't find an actual training of the final regression on the full dataset in the notebook. 
	So please check if this is correct. Otherwise you can just remove this table... and add your preferred analysis. also for the sort of bullet style i did for the analysis of the table, feel free to turn into paragraph if it takes too much space}

	In Table~\ref{tab:logreg_coefs}, we can observe the learned coefficients for each glass type. Let us be reminded: Positive values indicate that larger
	(standardized) values of a feature increase the probability of belonging to that class, while negative coefficients indicate the opposite. 

 	For the \textbf{BW-FP} class, the largest positive effect appears for \texttt{Mg} ($1.85$) and smaller positive contributions from \texttt{RI}, \texttt{Ba}, 
	and \texttt{Fe}, while \texttt{Na} and \texttt{Al} are strongly negative. This suggests that BW-FP glass is characterized by higher magnesium and lower aluminum 
	content relative to other types.  
	
	The \textbf{BW-NFP} coefficients are overall moderate, with \texttt{Na} and \texttt{Ca} being the main negative features, and \texttt{Fe} slightly positive.  
	
	The \textbf{VW-FP} class is dominated by large negative coefficients on \texttt{RI}, \texttt{Al}, and \texttt{Si}, and moderate positive contributions 
	from \texttt{Ca}, indicating that this glass type tends to have lower refractive index and aluminum content but slightly higher calcium. 

	For \textbf{containers}, the most pronounced coefficients are positive for \texttt{Al}, \texttt{Ca}, and \texttt{K}, and negative for \texttt{Na} 
	and \texttt{Mg}, distinguishing it from the window types.  

	The \textbf{headlamps} class exhibits strong positive coefficients across nearly all elements, but strong negative weight on \texttt{Ca}, 
	suggesting higher oxide content overall.  
	
	Finally, the \textbf{tableware} class displays an inverse pattern, with high negative coefficients for \texttt{K}, \texttt{Ba}, 
	and \texttt{Fe}, while \texttt{Na}, \texttt{Al}, and \texttt{Ca} are slightly positive.
	
	\input{figures/logreg_coefficients.tex}

	\section*{Closing Discussion}

	As detailed and discussed throughout the report, we applied and compared various machine learning models for regression and classification tasks on the 
	Glass Identification dataset. We discovered that regularized linear regression excelled at predicting the refractive index, 
	outperforming an artificial neural network and significantly outperforming the baseline model. This indicates that the relationship between chemical 
	composition and refractive index is well captured by a simple linear model with appropriate regularization.

	We where able to find a study that uses the same dataset, and tests different classification methods to identify Glass Types for forensic research. 
	The study by Goswami and Wegman \cite{GoswamiWegman2016}, found that the Decision Tree generally 
	outperformed the Logistic Regression. Their results showed that the Decision Tree handled the multiclass classification problem more effectively, 
	capturing the non-linear relationships between chemical attributes and glass types that Linear Regression could not model as easily. Linear Regression 
	performed reasonably well on simpler, binary distinctions, but showed reduced accuracy when extended to all six glass categories.

	In contrast, the results of this report indicate that both the Decision Tree and the Linear Regression perform similarly, with the latter 
	slightly outperforming the other in overall classification accuracy. This suggests that, while Decision Trees may better capture complex feature interactions, 
	the linear relationships in this dataset are strong enough for Linear Regression to achieve a competitive performance, or in this case, a marginally better
	one. Nonetheless, it is important to note that Linear Regressions are prone to overfitting, which might be the case here.


	\section*{Use of GenAI}

	Generative AI (ChatGPT by OpenAI and Claude by Anthropic) was used as a support tool during the project.

	Its role was limited to assisting with code debugging, latex table formatting, identifying and fixing minor
	implementation errors, and writing initial drafts for parts of the report which later had to be heavily 
	revised or completely redone to form a cohesive final report.

	All analytical choices, data processing steps, and result interpretations were made
	by the project members \textit{themselves} based on the actual model outputs and \textit{their} understanding of the
	underlying methods.


	\bibliography{citations}
	\bibliographystyle{unsrt}

	\vspace*{1cm}
	\appendix

	\LARGE\bfseries Appendix

	\normalsize\normalfont

	\section{Repository and supplementary materials}
	The full notebook, scripts, and generated figures for this project are available in the project repository:
	\begin{quote}
	\url{https://github.com/schependom/DTU\_machine-learning-projects/tree/main}
	\end{quote}
	This repository contains the data-loading and analysis code that produced the tables and figures cited
	above (see the \texttt{figures/} folder for the PDF outputs referenced in the report).


\end{document}
